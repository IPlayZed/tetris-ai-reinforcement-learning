{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-11T22:29:20.633218Z",
     "end_time": "2023-04-11T22:29:26.194114Z"
    }
   },
   "outputs": [],
   "source": [
    "from training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "trainer:Trainer = Trainer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T22:29:28.752255Z",
     "end_time": "2023-04-11T22:29:28.764448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87bbd3d4424b41e59feb3d1e97b963f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows-dev\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'models\\completed\\autosave' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlearning_timestamps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100_000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\git-clones\\tetris_gym_szte\\training\\trainer.py:56\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, learning_timestamps, show_progress_bar, save_frequency, save_name_prefix, save_name_suffix)\u001B[0m\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39mlearning_timestamps, progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[0;32m     47\u001B[0m                       callback\u001B[38;5;241m=\u001B[39mCheckpointCallback(\n\u001B[0;32m     48\u001B[0m                           save_freq\u001B[38;5;241m=\u001B[39msave_frequency,\n\u001B[0;32m     49\u001B[0m                           save_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_path_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/autosave\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     50\u001B[0m                           name_prefix\u001B[38;5;241m=\u001B[39msave_name_prefix\n\u001B[0;32m     51\u001B[0m                       ))\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model \\\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_path_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/completed/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_name_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlearning_timestamps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_name_suffix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluation_episodes: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10_000\u001B[39m, seed: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, save_videos: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     57\u001B[0m              video_episodes: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m seed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\git-clones\\tetris_gym_szte\\training\\trainer.py:64\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(self, evaluation_episodes, seed, save_videos, video_episodes)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation score: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(evaluate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_env, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model, evaluation_episodes)))\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m---> 64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m save_videos:\n\u001B[0;32m     65\u001B[0m     create_videos(env\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_env, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model, ep_num\u001B[38;5;241m=\u001B[39mvideo_episodes)\n\u001B[0;32m     66\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[1;32m~\\git-clones\\tetris_gym_szte\\tetris_gym\\utils\\eval_utils.py:16\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(env, model, ep_num)\u001B[0m\n\u001B[0;32m     13\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m---> 16\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     obs, reward, done, _ \u001B[38;5;241m=\u001B[39m env_test\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     20\u001B[0m     score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:535\u001B[0m, in \u001B[0;36mBaseAlgorithm.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    517\u001B[0m     observation: Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    521\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[np\u001B[38;5;241m.\u001B[39mndarray, Optional[Tuple[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[0;32m    522\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001B[39;00m\n\u001B[0;32m    524\u001B[0m \u001B[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;124;03m        (used in recurrent policies)\u001B[39;00m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:343\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    340\u001B[0m observation, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_to_tensor(observation)\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 343\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001B[39;00m\n\u001B[0;32m    345\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:687\u001B[0m, in \u001B[0;36mActorCriticPolicy._predict\u001B[1;34m(self, observation, deterministic)\u001B[0m\n\u001B[0;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation: th\u001B[38;5;241m.\u001B[39mTensor, deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    680\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;124;03m    Get the action according to the policy for a given observation.\u001B[39;00m\n\u001B[0;32m    682\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    685\u001B[0m \u001B[38;5;124;03m    :return: Taken action according to the policy\u001B[39;00m\n\u001B[0;32m    686\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 687\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:720\u001B[0m, in \u001B[0;36mActorCriticPolicy.get_distribution\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_distribution\u001B[39m(\u001B[38;5;28mself\u001B[39m, obs: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Distribution:\n\u001B[0;32m    714\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;124;03m    Get the current policy distribution given the observations.\u001B[39;00m\n\u001B[0;32m    716\u001B[0m \n\u001B[0;32m    717\u001B[0m \u001B[38;5;124;03m    :param obs:\u001B[39;00m\n\u001B[0;32m    718\u001B[0m \u001B[38;5;124;03m    :return: the action distribution.\u001B[39;00m\n\u001B[0;32m    719\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 720\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpi_features_extractor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    721\u001B[0m     latent_pi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor\u001B[38;5;241m.\u001B[39mforward_actor(features)\n\u001B[0;32m    722\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:141\u001B[0m, in \u001B[0;36mBaseModel.extract_features\u001B[1;34m(self, obs, features_extractor)\u001B[0m\n\u001B[0;32m    139\u001B[0m features_extractor \u001B[38;5;241m=\u001B[39m features_extractor \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures_extractor\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m features_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo features extractor was set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 141\u001B[0m preprocessed_obs \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_obs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservation_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize_images\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:135\u001B[0m, in \u001B[0;36mpreprocess_obs\u001B[1;34m(obs, observation_space, normalize_images)\u001B[0m\n\u001B[0;32m    133\u001B[0m     preprocessed_obs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, _obs \u001B[38;5;129;01min\u001B[39;00m obs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m--> 135\u001B[0m         preprocessed_obs[key] \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_obs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_obs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservation_space\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalize_images\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnormalize_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m preprocessed_obs\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\repos\\RL\\tetris_gym_szte\\Lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:111\u001B[0m, in \u001B[0;36mpreprocess_obs\u001B[1;34m(obs, observation_space, normalize_images)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normalize_images \u001B[38;5;129;01mand\u001B[39;00m is_image_space(observation_space):\n\u001B[0;32m    110\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m obs\u001B[38;5;241m.\u001B[39mfloat() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255.0\u001B[39m\n\u001B[1;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(observation_space, spaces\u001B[38;5;241m.\u001B[39mDiscrete):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# One hot encoding and convert to float to avoid errors\u001B[39;00m\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mone_hot(obs\u001B[38;5;241m.\u001B[39mlong(), num_classes\u001B[38;5;241m=\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mn)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(learning_timestamps=100_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
